{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6cd38e4-200c-4f21-a17c-e9d0a235a384",
   "metadata": {},
   "source": [
    "## Image Classification and Object Localization\n",
    "\n",
    "In this lab, you'll build a CNN from scratch to:\n",
    "  - classify the main subject in an image\n",
    "  - localize it by drawing bounding boxes around it.\n",
    "  \n",
    "You'll use the MNIST dataset to synthesize a custom dataset for the task:\n",
    "  - Place each \"digit\" image on a black canvas of width 75 x 75 at random locations.\n",
    "  - Calculate the corresponding bounding boxes for those digits.\n",
    "  \n",
    "The bounding box prediction can be modeled as a \"regression\" task which means that the model will predict a numeric value (as opposed to a category).\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d90206a-ff1a-4564-8dd2-0f141e6bbc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import os, re, time, json\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f6c113-f73a-4b88-8d01-d25b11772f1a",
   "metadata": {},
   "source": [
    "## Visualization Utilities\n",
    "\n",
    "These functions are used to draw bounding boxes around the digits.\n",
    "\n",
    "### Plot Utilites for Bounding Boxes [Run ME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "346991bf-ef82-45c0-a6ae-71ea29f29218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot Utilities for Bounding Boxes [RUN ME]\n",
    "\n",
    "im_width = 500\n",
    "im_height = 500\n",
    "use_normalized_coordinates = True\n",
    "\n",
    "def draw_bounding_boxes_on_image_array(image,\n",
    "                                       boxes,\n",
    "                                       color=[],\n",
    "                                       thickness=1,\n",
    "                                       display_str_list=()):\n",
    "  \"\"\"Draws bounding boxes on image (numpy array).\n",
    "  Args:\n",
    "    image: a numpy array object.\n",
    "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
    "           The coordinates are in normalized format between [0, 1].\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list_list: a list of strings for each bounding box.\n",
    "  Raises:\n",
    "    ValueError: if boxes is not a [N, 4] array\n",
    "  \"\"\"\n",
    "  image_pil = PIL.Image.fromarray(image)\n",
    "  rgbimg = PIL.Image.new(\"RGBA\", image_pil.size)\n",
    "  rgbimg.paste(image_pil)\n",
    "  draw_bounding_boxes_on_image(rgbimg, boxes, color, thickness,\n",
    "                               display_str_list)\n",
    "  return np.array(rgbimg)\n",
    "  \n",
    "\n",
    "def draw_bounding_boxes_on_image(image,\n",
    "                                 boxes,\n",
    "                                 color=[],\n",
    "                                 thickness=1,\n",
    "                                 display_str_list=()):\n",
    "  \"\"\"Draws bounding boxes on image.\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
    "           The coordinates are in normalized format between [0, 1].\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: a list of strings for each bounding box.\n",
    "                           \n",
    "  Raises:\n",
    "    ValueError: if boxes is not a [N, 4] array\n",
    "  \"\"\"\n",
    "  boxes_shape = boxes.shape\n",
    "  if not boxes_shape:\n",
    "    return\n",
    "  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
    "    raise ValueError('Input must be of size [N, 4]')\n",
    "  for i in range(boxes_shape[0]):\n",
    "    draw_bounding_box_on_image(image, boxes[i, 1], boxes[i, 0], boxes[i, 3],\n",
    "                               boxes[i, 2], color[i], thickness, display_str_list[i])\n",
    "        \n",
    "def draw_bounding_box_on_image(image,\n",
    "                               ymin,\n",
    "                               xmin,\n",
    "                               ymax,\n",
    "                               xmax,\n",
    "                               color='red',\n",
    "                               thickness=3,\n",
    "                               display_str=None,\n",
    "                               use_normalized_coordinates=True):\n",
    "  \"\"\"Adds a bounding box to an image.\n",
    "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
    "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    ymin: ymin of bounding box.\n",
    "    xmin: xmin of bounding box.\n",
    "    ymax: ymax of bounding box.\n",
    "    xmax: xmax of bounding box.\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: string to display in box\n",
    "    use_normalized_coordinates: If True (default), treat coordinates\n",
    "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
    "      coordinates as absolute.\n",
    "  \"\"\"\n",
    "  draw = PIL.ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                  ymin * im_height, ymax * im_height)\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom),\n",
    "             (right, top), (left, top)], width=thickness, fill=color)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95659471-c169-4e24-af84-b0eebced8c22",
   "metadata": {},
   "source": [
    "These utilites are used to visualize the data and predictions\n",
    "\n",
    "## Visulaiztion Utilities [RUN ME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b1472f-67aa-4a2b-a82d-148c7fa8efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualization Utilities [RUN ME]\n",
    "\"\"\"\n",
    "This cell contains helper functions used for visualization\n",
    "and downloads only. \n",
    "\n",
    "You can skip reading it, as there is very\n",
    "little Keras or Tensorflow related code here.\n",
    "\"\"\"\n",
    "\n",
    "# Matplotlib config\n",
    "plt.rc('image', cmap='gray')\n",
    "plt.rc('grid', linewidth=0)\n",
    "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
    "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
    "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
    "plt.rc('text', color='a8151a')\n",
    "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
    "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
    "\n",
    "# create digits from local fonts for testing\n",
    "def create_digits_from_local_fonts(n):\n",
    "  font_labels = []\n",
    "  img = PIL.Image.new('LA', (500*n, 500), color = (0,255)) # format 'LA': black in channel 0, alpha in channel 1\n",
    "  font1 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'DejaVuSansMono-Oblique.ttf'), 25)\n",
    "  font2 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'STIXGeneral.ttf'), 25)\n",
    "  d = PIL.ImageDraw.Draw(img)\n",
    "  for i in range(n):\n",
    "    font_labels.append(i%10)\n",
    "    d.text((7+i*500,0 if i<10 else -4), str(i%10), fill=(255,255), font=font1 if i<10 else font2)\n",
    "  font_digits = np.array(img.getdata(), np.float32)[:,0] / 255.0 # black in channel 0, alpha in channel 1 (discarded)\n",
    "  font_digits = np.reshape(np.stack(np.split(np.reshape(font_digits, [75, 75*n]), n, axis=1), axis=0), [n, 75*75])\n",
    "  return font_digits, font_labels\n",
    "\n",
    "\n",
    "# utility to display a row of digits with their predictions\n",
    "def display_digits_with_boxes(digits, predictions, labels, pred_bboxes, bboxes, iou, title):\n",
    "\n",
    "  n = 10\n",
    "\n",
    "  indexes = np.random.choice(len(predictions), size=n)\n",
    "  n_digits = digits[indexes]\n",
    "  n_predictions = predictions[indexes]\n",
    "  n_labels = labels[indexes]\n",
    "\n",
    "  n_iou = []\n",
    "  if len(iou) > 0:\n",
    "    n_iou = iou[indexes]\n",
    "\n",
    "  if (len(pred_bboxes) > 0):\n",
    "    n_pred_bboxes = pred_bboxes[indexes,:]\n",
    "\n",
    "  if (len(bboxes) > 0):\n",
    "    n_bboxes = bboxes[indexes,:]\n",
    "\n",
    "\n",
    "  n_digits = n_digits * 255.0\n",
    "  print(n_digits.shape)\n",
    "  n_digits = n_digits.reshape(n, 500, 500, 3)\n",
    "  fig = plt.figure(figsize=(20, 4))\n",
    "  plt.title(title)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  \n",
    "  for i in range(10):\n",
    "    ax = fig.add_subplot(1, 10, i+1)\n",
    "    bboxes_to_plot = []\n",
    "    if (len(pred_bboxes) > i):\n",
    "      bboxes_to_plot.append(n_pred_bboxes[i])\n",
    "    \n",
    "    if (len(bboxes) > i):\n",
    "      bboxes_to_plot.append(n_bboxes[i])\n",
    "\n",
    "    img_to_draw = draw_bounding_boxes_on_image_array(image=np.uint8(n_digits[i]), boxes=np.asarray(bboxes_to_plot), color=['red', 'green'], display_str_list=[\"true\", \"pred\"])\n",
    "    plt.xlabel(n_predictions[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    if n_predictions[i] != n_labels[i]:\n",
    "      ax.xaxis.label.set_color('red')\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.imshow(img_to_draw)\n",
    "\n",
    "    if len(iou) > i :\n",
    "      color = \"black\"\n",
    "      if (n_iou[i][0] < iou_threshold):\n",
    "        color = \"red\"\n",
    "      ax.text(0.2, -0.3, \"iou: %s\" %(n_iou[i][0]), color=color, transform=ax.transAxes)\n",
    "\n",
    "\n",
    "# utility to display training and validation curves\n",
    "def plot_metrics(metric_name, title, ylim=5):\n",
    "  plt.title(title)\n",
    "  plt.ylim(0,ylim)\n",
    "  plt.plot(history.history[metric_name],color='blue',label=metric_name)\n",
    "  plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73cfe2c4-7240-44dd-ad88-55e4fdbc1e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 16:14:24.869980: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-12-21 16:14:24.870984: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices(\"GPU\"))\n",
    "\n",
    "strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4721c-b37f-4d5a-aa69-96328c83decd",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "The global batch size is the batch size per replica (64 in this case) times the number of replicas in the distribution strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167566de-2c1a-456f-b343-6f93b73302d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 * strategy.num_replicas_in_sync # Gobal batch size.\n",
    "# The global batch size will be automatically sharded across all\n",
    "# replicas by the tf.data.Dataset API. A single TPU has 8 cores.\n",
    "# The best practice is to scale the batch size by the number of\n",
    "# replicas (cores). The learning rate should be increased as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17741dc-be8e-4410-a2fb-491a23bcd90b",
   "metadata": {},
   "source": [
    "## Loading an Preprocessing the Dataset\n",
    "\n",
    "Define some helper functions that will pre-process your data:\n",
    "  - `read_image_tfds`: ramdomly overlays the \"digit\" image on top of a larger canvas.\n",
    "  - `get_training_dataset`:loads data and splits it to get the training set.\n",
    "  - `get_validation_dataset`: loads and splits the data to get the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c264759-be4a-4e8b-b20d-338e6ecbb9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This should be updated\n",
    "root_dir = '/Users/lucas/Data_Science_Portfolio/M&M project/M&M_images_cropped'\n",
    "\n",
    "validation_dir = os.path.join(root_dir, 'validation')\n",
    "training_dir = os.path.join(root_dir, 'training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ddeebc91-d13b-4af2-bdc9-3812ae601dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PIL.Image.open(os.path.join(training_dir, 'M&Ms/4.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9aa4dfb-00a2-4602-8b37-2f423944e6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.78823529 0.74509804 0.65882353]\n",
      "  [0.79215686 0.74901961 0.6627451 ]\n",
      "  [0.79215686 0.74901961 0.6627451 ]\n",
      "  ...\n",
      "  [0.6627451  0.61960784 0.54117647]\n",
      "  [0.65882353 0.61568627 0.5372549 ]\n",
      "  [0.65882353 0.61568627 0.5372549 ]]\n",
      "\n",
      " [[0.79215686 0.74901961 0.6627451 ]\n",
      "  [0.79607843 0.75294118 0.66666667]\n",
      "  [0.8        0.75686275 0.67058824]\n",
      "  ...\n",
      "  [0.6627451  0.61960784 0.54117647]\n",
      "  [0.66666667 0.62352941 0.54509804]\n",
      "  [0.66666667 0.62352941 0.54509804]]\n",
      "\n",
      " [[0.78431373 0.74117647 0.65490196]\n",
      "  [0.78823529 0.74509804 0.65882353]\n",
      "  [0.79607843 0.75294118 0.66666667]\n",
      "  ...\n",
      "  [0.6627451  0.61960784 0.54117647]\n",
      "  [0.6627451  0.61960784 0.54117647]\n",
      "  [0.66666667 0.62352941 0.54509804]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.78431373 0.74117647 0.6627451 ]\n",
      "  [0.78039216 0.7372549  0.65882353]\n",
      "  [0.78039216 0.7372549  0.65882353]\n",
      "  ...\n",
      "  [0.69411765 0.6627451  0.58823529]\n",
      "  [0.69411765 0.6627451  0.58823529]\n",
      "  [0.69411765 0.6627451  0.58823529]]\n",
      "\n",
      " [[0.78431373 0.74117647 0.6627451 ]\n",
      "  [0.78039216 0.7372549  0.65882353]\n",
      "  [0.78039216 0.7372549  0.65882353]\n",
      "  ...\n",
      "  [0.69803922 0.66666667 0.59215686]\n",
      "  [0.69803922 0.66666667 0.59215686]\n",
      "  [0.69803922 0.66666667 0.59215686]]\n",
      "\n",
      " [[0.78039216 0.7372549  0.65882353]\n",
      "  [0.78039216 0.7372549  0.65882353]\n",
      "  [0.78039216 0.7372549  0.65882353]\n",
      "  ...\n",
      "  [0.69803922 0.66666667 0.59215686]\n",
      "  [0.69803922 0.66666667 0.59215686]\n",
      "  [0.69803922 0.66666667 0.59215686]]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "\n",
    "numpydata = asarray(image)\n",
    "\n",
    "print(numpydata/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0242c39-d31d-48d4-8adc-8a6d2ba96815",
   "metadata": {},
   "source": [
    "Notice that the array is not normalized so we will have to divide by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b871d724-a90e-4a70-bd43-4bc2a684c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_and_ms = os.path.join(training_dir, 'M&Ms')\n",
    "nots = os.path.join(training_dir, 'Not_M&Ms')\n",
    "\n",
    "training_images = np.array([])\n",
    "training_labels = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd907626-2282-4b3a-8ea6-a6331c2df02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(m_and_ms):\n",
    "    if i != '.DS_Store':\n",
    "        image = PIL.Image.open(os.path.join(m_and_ms, i))\n",
    "        numpydata = asarray(image)\n",
    "        normalized = numpydata/255\n",
    "        training_images = np.append(training_images, normalized)\n",
    "        training_labels = np.append(training_labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5367be0c-e952-4060-b977-7577b2c230e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "450000\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(m_and_ms)))\n",
    "print(len(training_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dcaa1845-03f8-4a39-b896-0b8d879ed07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(nots):\n",
    "    if i != '.DS_Store':\n",
    "        image = PIL.Image.open(os.path.join(nots, i))\n",
    "        numpydata = asarray(image)\n",
    "        normalized = numpydata/255\n",
    "        training_images = np.append(training_images, normalized)\n",
    "        training_labels = np.append(training_labels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02eb8de8-415e-4884-b367-7a12facb0e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(nots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5862dec-d338-4676-a07e-ea9f8b1137ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = np.reshape(training_images, (24, 100, 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc9ea9cd-8ee9-4f2c-beb4-e8867cf304f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "padded = tf.pad(training_images[0], [[100, 200], [100, 200], [0, 0]], mode=\"CONSTANT\", constant_values=0.9)\n",
    "image_pil = PIL.Image.fromarray(np.uint8(padded*255))\n",
    "\n",
    "image_pil.save(\"confused.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20adf1d3-3893-424b-b1e2-70eaf7bc629f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 500, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "train = np.array([])\n",
    "training_bboxes = np.array([])\n",
    "\n",
    "for i in training_images:\n",
    "    xmin = tf.random.uniform((), 0, 400, dtype=tf.int32)\n",
    "    ymin = tf.random.uniform((), 0, 400, dtype=tf.int32)\n",
    "    xmax = xmin + 100\n",
    "    ymax = ymin + 100\n",
    "    image = tf.pad(i, [[xmin, 500 - xmax], [ymin, 500 - ymax], [0,0]], mode=\"CONSTANT\", constant_values=0.9)\n",
    "    train = np.append(train, image)\n",
    "    training_bboxes = np.append(training_bboxes, [[xmin, ymin, xmax, ymax]])\n",
    "   \n",
    "train = np.reshape(train, (24, 500, 500, 3))\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b47e3314-e7c1-4e7c-9ac9-33ca8968f1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 4)\n",
      "(24, 500, 500, 3)\n",
      "(24,)\n"
     ]
    }
   ],
   "source": [
    "training_bboxes = np.reshape(training_bboxes, (24, 4))\n",
    "print(training_bboxes.shape)\n",
    "\n",
    "print(train.shape)\n",
    "print(training_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a827ea7-613d-4418-8a0c-01cba78ef6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_and_ms = os.path.join(validation_dir, 'M&Ms')\n",
    "nots = os.path.join(validation_dir, 'Not_M&Ms')\n",
    "\n",
    "validation_images = np.array([])\n",
    "validation_labels = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "908675ee-41df-4c72-a23e-47067e7b092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(m_and_ms):\n",
    "    if i != '.DS_Store':\n",
    "        image = PIL.Image.open(os.path.join(m_and_ms, i))\n",
    "        numpydata = asarray(image)\n",
    "        normalized = numpydata/255\n",
    "        validation_images = np.append(training_images, normalized)\n",
    "        validation_labels = np.append(training_labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f980745a-6262-498a-92f8-9461d5ad3722",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(nots):\n",
    "    if i != '.DS_Store':\n",
    "        image = PIL.Image.open(os.path.join(nots, i))\n",
    "        numpydata = asarray(image)\n",
    "        normalized = numpydata/255\n",
    "        validation_images = np.append(training_images, normalized)\n",
    "        validation_labels = np.append(training_labels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62c4f20c-7159-4f2e-b487-d70ab4000951",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_images = np.reshape(validation_images, (25, 100, 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "651127e4-4aa6-48cd-850d-6be652d53aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 500, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "validate = np.array([])\n",
    "validation_bboxes = np.array([])\n",
    "\n",
    "for i in validation_images:\n",
    "    xmin = tf.random.uniform((), 0, 400, dtype=tf.int32)\n",
    "    ymin = tf.random.uniform((), 0, 400, dtype=tf.int32)\n",
    "    xmax = xmin + 100\n",
    "    ymax = ymin + 100\n",
    "    image = tf.pad(i, [[xmin, 500 - xmax], [ymin, 500 - ymax], [0,0]], mode=\"CONSTANT\", constant_values=0.9)\n",
    "    validate = np.append(validate, image)\n",
    "    validation_bboxes = np.append(validation_bboxes, [[xmin, ymin, xmax, ymax]])\n",
    "   \n",
    "validate = np.reshape(validate, (25,  500, 500, 3))\n",
    "print(validate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4e6a555-de97-47d0-8d5c-e37a3e2157a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 4)\n",
      "(25, 500, 500, 3)\n",
      "(25,)\n"
     ]
    }
   ],
   "source": [
    "validation_bboxes = np.reshape(validation_bboxes, (25, 4))\n",
    "print(validation_bboxes.shape)\n",
    "\n",
    "print(validate.shape)\n",
    "print(validation_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3119f51-354a-4d2e-b53e-570f21fdbe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1000, 1000, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABiIAAAFcCAYAAABFraaEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA2L0lEQVR4nO3debxdZX0v/s9ae+8zJCc5mcMQIAxhFBEQkFoVW9r+Oljt4O29Hb22VlvqUAecWjvZ9rb31qrU2Vpsa9urtdqqVasWh14FqiCTgBSQACGBJGQ6OdPea/3+OEkgkgnIOicnvN8v8gpn5Vl7P3vnm+essz77eZ5i06ZNdQAAAAAAABpQznQHAAAAAACAw5cgAgAAAAAAaIwgAgAAAAAAaIwgAgAAAAAAaIwgAgAAAAAAaIwgAgAAAAAAaIwgAgAAAAAAaIwgAgAAAAAAaIwgAgAAAAAAaIwgAgCAx+XBa67NbW//i0xu2dLI419/6RvyxYsufkzn3vPRj+Uzq07P9nvuPci9euxue/tf5DOrTt/t2BcvujjXX/qGx/R4e3p/bn/Xe7Luc59/zH2cSQf69736Q3+fez76sUcc33DV1fnMqtOz9tOfPWh9Ohh1tKe/9wN1/aVvyOfOOvcxP/feHvOx/rsCAIBHSxABAMDjsumaa3P7Ze/M5JatjTz+SZe8JOe84+2P6dxlFz0rT/vw32dg6dKD3KuD65x3vD0nXfKSx3Tunt6fO9793qz73L8fjK4dslZ/6O9z7z99fFqea7bUEQAAHKraM90BAACeWHpjY2kNDBxw+znHHfuYn6tv8aL0LV70mM+fLvPPeGyflE8e3/vDgTnQOuqNjqY1ODgNPQIAgNlFEAEAwGN229v/Irdf9s4kyZef/QO7jp/3t5dn8QXn54sXXZx5q1bl6J/+ydz+jndl5PY7ctwLfjGnvOaVuetv/y5rP/XpjNxxZ3qjoxk8ZkWOeu6PZ+X//MWUnc6ux7r+0jdk49VX56IvPrTU0GdWnZ5jf/5nM3zWk3PHu96b0TVrMnflcVn1my/Psu+7aFe7ez76sdz4ujfmmVd8LnNWHJ0kuernfimTDz6YM//XH+aWP/6TbL7xW+lfsiTH/Pfn5/gX/XKK8qFJw1tvuy23/OGf5MFvXJPW4GCO/JH/L0ue9cxc86u/tus17sv9V3wpt73lrdl2+x0ZWL4sx/7c/9hjuy9edHEWnX9+nvynf/Son/u735+dy/+s+djHs+ZjH0+SLDz/vFzwoQ+mNzqa2952WdZ99nMZv/+BtAYHMnjMMVn5whfkqOf86F5fx8SGjbnt7X+RjVddnbH77ktrYDBDJ6/KSS+7JIvOe+qudtvvuTdffvYP5JTXvjopy6z+mw9lYuPGDJ18ck57w+uy4Oyzdnvcez76sdzxnvdl9J57M+eYY3LCS160z/fz4e/X2L1rdnu9A0cftVuNVN1uvv2Wt+bej34s3ZHtWXDWmTntd347Qyccv9tjrf9/X80d73lfNl9/Y+peL/NPPy2rXv4bWfw9F+7Wz73V0em/+6Z8+/+8JVtuviXLvv/Zecpb/+yAXkOS3PepT+eej3w027797Uxu2ZrBo4/Osou/Lyde8pK058x5RPutt92Wm//gj7L5m9enHBjIkT/6wznl0lftFn7UdZ27/+4fcvc/fDgjd34nZX9/Fl/4tJxy6asy59hj9tmftZ/+TO58/19l5I47U3W76V+yJIsuOC9n/q8/PODXBAAAeyKIAADgMTvm+T+dyU2bs/pvPpSz3/H29O9YumbopBN3tdnyrW9l25/ckRN//cUZPGZF2jtumm5ffXeOfM6PZvCYFSk7nWy9+dbc/q73ZOSOOw7oxucDX/xSNl9/Q1a94jfSmjMnd77vA7n2kpflGZ/91H5vuI6vX5/rXnVpVr7wBTnxN34993/uC/n2//nz9C9blqN/4rlJkrH7H8jVP/dLaQ0O5ozfe1P6Fi/KfZ/819z8+28+oPdmw1e/lmt/7Tey4Oyz8pS3/lnqXi93vv8DGV+/fr/nPp7nftqH/z5X/+L/zOILzs+Jl/xakqQ9NDdJcssf/UnW/PMnsuo3X5Z5p5+W3vbRbPv2bZnctGmfjzmxeXOS5KSXXpL+JUvS3b4993/u87n651+Q8/76A48IZFZ/6O8z94Tjc+obX5ckue2tl+UbL3pxnnnF59KZNy/JQzf3l138fTn1dZemu3Vr/uuyd6SamEjKfa8ge8473p5rX/qb6cwbyum/+6YkSdnX2a3NbW95axacc3ae9Ie/n+62kdz6v/8s17z41/OMz3wyRauVJFnzz/+S61/z+iy7+Pvy5D/94xTtdu7+hw/n6y/81Tz1A+/dLYzYk/EHHsj1r740x7/ol7PqVa9IUTy6lW+3f+euLH3WM7PyBb+Q1uCcbLvjjtz53r/M5utvyPl/81e7ta0mu/nGr7wkx/z3/5YTfvVF2XTttbn9ne/J6L1rcu5737mr3U2//bu5958+luN+8edz8qWvyuSmzbn9L96VK3/mZ/P0T3ws/UuW7LEvD177zXzz5a/KET/ywznpZZek7O/P6L1rsvHKqx7VawIAgD0RRAAA8JgNHHlEBo86Mkky7/TTdn1a/OEmNmzM9376E5l7/Mrdjp/2htfu+v+6qrLwqeems3BBbnzdG3Pq6y9NZ3h4n8/dGxvLeR/8wK6b7PPPOD1XPP2irP30Z3LCi/f9yfrJBzfl3Pe9OwvOenKSZMnTvycbr7o6933iU7uCiLv+6oOZ3LQ5F3zorzO06qQkydJnPTNff+GvZvQANi3+9lvelr4li/PUy/8yrf7+qed5xvfmS8/e/wbBj+e5F5x9VoqySGfRokfMQHjwmmuz+Hu/Jyv/5y/tOrbs2c/ab3+GTjg+Z/zem3Z9Xfd6WfKMp2f03nuz+oN/+4ggojV3bs5977t23fAfWLYsX/upn8n6L30lR/7Yj6Suqtz252/L/DNOz9nvvCxFUSRJFj71nHz5B344/cuW7bM/8884Pa2B/rSGhh7xGnf1+cQTc9af/emur4tWmW++7JXZfP2NWXD2WemNjubmN/9xlj77opzzzst2tVt60TPz1ef+VL79Z2/NhfsJIiY3bc5T3v7nWXzh0/bZbm9OfNi+IHVdZ8G5Z2foxBNz9c/9YrbecmvmnXrKQ38+OZmVL3xBVv7SLyRJlnzv96Rot3PbW96WB79xTRaee042XXtd7vm/H8kpr780x7/wBbvOXfTUc/PlH/yRfOcDH8wpl75qj33ZdM21SV3njD/4nV1hUZKs+KmfeEyvDQAAHs5m1QAANGreKSc/IoRIki03fSvfePEl+cJ5F+azpzwp/3bak3PDa16XutfLyJ3f2e/jLrrggl0hRJL0L1mS/sWLMrpjyZ596V+6ZFcIsdPQKafsdu7Gq/8zQyev2hUE7HTkj/3Ifh+/u317Nt9wY5b/4MW7QohkambC0mc/e7/nP57n3pfhJ5+Z9V/6Sm7932/JhquuTm9s7IDPXf13/5CvPven8m9nPCWfPfXM/NtpT86Gr16Zbbff8Yi2Sy965q4QIknmnTJ1Q33n+ztyx50ZX3d/jnzOj+4KIZJk8Oijs+Dssx/ry9u9D9+/+/s8tLMPa6b68OA112Zy0+Yc/RPPTdXt7vpVV1WWPPMZ2XzDjelu377P5+gMz3/MIUQyNSvout98Tf79wmfs+jdw9c/9YpLs8X096sefs9vXRz7nx5IkG6+8Okly/xVfTIoiRz33Obu9pr6lSzLv1FOy8aqr99qX4TPPTJJ882WvzH3/+umMrV33mF8XAAB8NzMiAABo1M7lmh5udM2aXPWzv5C5xx+f037r9Rk8+uiU/f3ZfP0N+dbv/kF64+P7fdy+hQsecazo60tvbP/ndhY88tyyry+98YduzE9u2pTBFSse+bxLFu/38bubtyRVtcdlcPqX7nlpnId7PM+9L6f99hsycMTyrP3Xz+TO974/ZX9/ljzj6Tnlta/O3JUr93renR+4PLf+8Z/mmP/xMznpFS9L38IFKVqt3PbWt+/xhnnfd72/ZX9fkux6f3cuBbW392f03v3PONmfR/Shb0cfdoQvE+s3JEm++dJX7PUxJjdv3uNeDTvtqbYPVHdkJFf97C+k7OvPqt98eeauXJnW4EDG7lubay952SNCoqLdfkTN76yliR3v58SGDUld54qnPWOPzzl4zN6XLFt0/lNz9rsuy11//be54TWvTzUxkaFVJ+WEX3vxPvcPAQCAAyGIAACgWQ/7xPtO6z73hfS2j+bsd7wtg0c/tJzTlptvmc6e7VNnwYKMb9jwiOPjD+x/j4f28PykKPa4H8SBnP94nnuf/ZozJ6te/tKsevlLM75+fR740lfy7f/zllzz4kvyjM9+aq/n3ffPn8iiC87PGb//O7sd747se8bA3uwMgh7r+3MwdBYtTJKc9qY3ZsFT9ry8U//i/QQ/e6jtA7Xxyqsyvu7+nP+3H8yiC87bdXxyy5Y9tq+73Uw8uGm3MGLne7UzdOksXJAURS74+7/ZFbw83J6OPdzyi78/yy/+/lTjE9n0zetyx3vel+tf+ZoMrjg6C89+yqN6fQAA8HCWZgIA4HHZeXOzehTL/OxcjufhN0brus49//cjB7dzj8Oi88/Ltm/flm23/ddux9d+6tP7Pbc9Z06Gn3xm1v3b53eb3dHdNpIHrrii0edOpt7Xanzffx/9S5ZkxU/9RI78sR/NyB13pjc6uvfGRfGIm9hbb7k1m6795gH157vNPeH49C9bmvs++a+p63rX8dF7782ma689oMco+/oeVc19t4XnnJ32/PnZ9l+3Z/jMJ+3x1/5u3D8+j/w3kCR3/8OH93rGmn/5xG5f3/eJTybJriBj2bMvSuo6Y+vW7fH1zDvl5APqWdnfl0UXnJeTX/PKJMnWb918QOcBAMDemBEBAMDjMrTj5uZdH/ybHPWTz0vZbmfu8cfvtn/Dd1v89O9J0enkut98TY5/0QtTjU9k9d/9w14/DT4TjnvBL+aej/5Tvv4rL86ql780fUsW575PfCrb7phaiqgo9v2ZnlWveFm+/su/mq+/4Jez8oUvSN2rcuf7/jKtwcFMbtrc6HMPnXxyNl71n7n/C1ekf9nStObOzdAJx+drP/UzWfrsizLv1JPTmT+cbbffnjX//C9ZcPZT0hoc3OvjLX32s3L7O96d2952WRadf15G7vhObn/HOzNnxdGper199mVPirLMqle8LDe+4bdz7a+/NCv+2/PT3bIl/3XZO/a4XNPeXuPaT/1r7vvUpzPnmBUp+/sP+EZ7krTnzs3pb3pDrr/0DZncvDlH/NAPpm/x4kxs3Jitt9yaiY0bHzED5GBacM7Z6QzPz01v+t2c9NJLUrTbWfMvn8zWW27dY/ui08l3PnB5etu3Z/jMM7Pp2mtz+zvfkyXPekYWPvXcJMnCc8/Jip95fm583W9lyw03ZeF5T01rzmDG738gD37jmsw7+eQc+3P/fY+Pf9tbL8vY2rVZfOHTMnDEEZncuiV3ffBvU3TaWXj+Uxt7HwAAeGIQRAAA8LgsvuD8nPCSF+Xej/1z7v7wPyZVlfP+9vIsvuD8vZ4zdOIJOfsv3pbb/vxtufaSl6ezYEGOes6PZuULfynf+OUXT2Pv925g+bKc/6EP5pY3/6/c9KbfS2twIMt/4OKsevlLc8Olr09n/rx9nr/ke78n57zzstz21rfnmy9/VfqXLsmxP/s/0hsfy+2XvbPR5z7tt16fb/3em3Pdb746vdHRLDz/vFzwoQ9m0YUX5P5/vyJ3Xf7B9EbH0r98eY563nNz4q/t+z0/8ddenN7oWO75yD/lzvd9IEMnnZjTf/93su7fvpCNV+99A+R9WfH8n0qS3PHe9+faS16WwRVH54SX/Go2Xv31A3rMVS+/JOMPPJAb3/im9EZGMnD0Ubnoi59/VH046rk/noEjj8qd7/vL3PSm3013ZCR9ixZn/mmn5uiffN5jeVkHrG/hgpzzvnfn1j/+01z/6temNTiYZRd/X57y1j/LV5/3049oX3baOfe978rNf/BHuf2d70lroD8r/ttP55TXvnq3dk968+9lwVPOyt3/8OGs/ru/T13VGVi2NAvOPSfDZ5251/4Mn/XkbL7xxtz6v9+SiY0b05k/P8NPOiPn//VfZd6qVQf99QMA8MRSbNq0qd5/MwAAIElu/K3fyX2f/FS+/+qvNrx0z6H13AAAAI+VGREAALAX/3XZO9O/fFnmHLMive3bc/8VX8o9H/7HnPjrL2k8CJjJ5wYAADiYBBEAALAXRaedO9//gYyvXZeq283clcfl1Ne/Nse94BcO6+cGAAA4mCzNBAAAAAAANKac6Q4AAAAAAACHL0EEAAAAAADQGEEEAAAAAADQGEEEAAAAAADQmPaBNhweHm6yHwAAAAAAwCyzefPm/bY54CAiScbHxx9zZwAAAAAAgMNHf3//AbWzNBMAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANAYQQQAAAAAANCY9qNp3N/f31Q/mGXGx8en5XnUHA83HXWn5vhu6o7p5nssM8FYx3Qz1jETjHXMBHXHdPM9lkPVowoi1q5dm7qum+oLs0BRFDniiCOm9TnVHdNdd2qORN0x/XyPZSYY65huxjpmgrGOmaDumG6+xzITHk3dPaogoq5rxcW0U3dMNzXHTFB3zAR1x3RTc8wEdcd0U3PMBHXHTFB3PBqPKogAAAAAAA5P27Zuzv333b3jqyLJ1E3mqttLNTGRotNO0Wrt+LOpG9H9AwNZcdxJKUtb0QJ7J4gAAAAAADK2fSRr7r4zSZ06dbauW5f7rr8pG+64K+Obt6Y10JelJ6/KcRecl9bcwaRO5i9YmKOPPXGmuw4c4gQRAAAAAEDqZMdSO1XuvPI/c/uXv5Le6GjqyV7qyV6qus7WNWvzwH99J0/6yeekf8FwupPd7Jw5AbA35kwBAAAAAFPzIHq93H3NN3Pz5z6fTExmzqIlWXT8yrT7+9Lpa2dw7tyM3r8ut3z2C5kcHUtVCSGA/TMjAgAAAABI6jqjm7bk1n//crpjE5l7xBEpO33pDA5m6apV6W3bmrIsM7Jlax78zp3ZdO/azF+0eKZ7DcwCZkQAAAAAAEmd3Hvzzdmy4YFUdZWq10vVm8zI5q0ZGx/PtpFtWb92bUa2bU1vfDwb1qxNr9ub6V4Ds4AZEQAAAABAer1eNqxenaROr+pl64b1GVq8JK2yzuiWBzO+aVOK1OlVRbp1Mjo6lrGxcVtEAPtlRgQAAAAAkKqqMjE5nhRFylYrvV43o5s2pTs+mpGNG9PrdVPVSa+qUpVF2nPnpter5BDAfpkRAQAAAACkqrrpLBxKN3XadZWkyNjoSEZHtqTu1alTp9UqUhdFWvOH0z88nLJVppjpjgOHPEEEAAAAAJDUVYaWL0wxdzDjm7alLIv0ulWqqkpd1ynqOmXdSwYGsvDoI9M3dzB9fZ1IIoD9sTQTAAAAAJCiLDNn/lAWnnRMJspksltlrNvNWK+X8V4v43Uy3mml78ilWbDiqMwbGszg4EAkEcD+mBEBAAAAAKTdbmXuvLlZfuKxGR8dy4N3rUu9fSxV1UuKIp25g5m7dFEWHntsFh65PPOH56W/vzPT3QZmAUEEAAAAAJBWq5158xdkYnwsR568MnMWLczWLVtS9XopyzLtvv7MHV6YZUcfm+VHHJH5C+al1XF7Edg/IwUAAAAAkME587LypLOy/MiV2bZ1a7aPjGZ8bCxVVaeu63T6+jI0f2HmL1ycuUPz0mq30tfXn6KwNBOwb4IIAAAAACBz5y3IKWdeONPdAA5DgggAAAAAwMwGoDHlTHcAAAAAAAA4fAkiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxggiAAAAAACAxrRnugMAAAAAAMDB1e1O5v77VqeuqqQopg7WdZJi94bFjiN1krLM0iNWpNPpO6h9EUQAAAAAAMBhpjs5kdW335xer5skqet6x58UKYoiU8lDUhRFinrqz4tWKwsWLRFEAAAAAAAA+1fX9a4Aoq7rqdChSJIqZVlOZRHFw9rVySNmTBwEgggAAAAAADgM1UmqqkqSFGWdImWKukpRFilSTbWokyp1qqJO2dC20oIIAAAAAAA4zNR5+ASHOkVdp6gm0+p2k243VVGnv92XqtNJUqSueqlT7thH4uASRAAAAAAAwGFpatZDWSR9o+Pp3nZTJm7/dia3bU5RJJOdvpSLl6b/6ONSHHNiunNaqSOIAAAAAAAADkBRT/3KhvUZ+doVyZq7UvcmUtRFirJIXZSpHtyY8TvvSH3k7Wld+KxGZkQ0s+ATAAAAAAAwY4o6Kes61QPrsvWKT6e75jtJ3UuS1EVSlGXKdjtlpzWVFKxbnd43r0rR7R70vpgRAQAAAAAAh5065cRYxq66IsUD96aqqvTKJHWdvk5fhuYNpdXpS6vTl/HxiYxuG0nWrUk9PnbQeyKIAAAAAACAw9DoXbene9/dO2Y51KlTpFUUKTutVClS1HXS66UokqJIJiZGk54ZEQAAAAAAwH7UvV623XJz6vGJ1HWVIlPbP/TqZPv2sYyPTyZFUqRIq9VOd7Kbuq+TuoE9IgQRAAAAAABwmOmNj2Vi0/q0U6coi6RO6hSpk9RVL1Vdpa4zFTwUE0mSstUSRAAAAAAAAPtXV1XqbjdVipR1ktSp63rXzIgUdeoqqeo6KacCiqKqMtXi4BJEAAAAAADAYabV6cui085Ke3RbyiSpqx1LMSWppmZCVFWVuixSFEXqokjdNzflwOBB78shFURUVS+9Xne/eUudPNSmKNJqdVIUBz+lAQAAAACA2WhwwaI86/V/+uhPLMqD3pdDKogY2bYpG9evTlI/FCzs2Cxjx4SRHf9NfVUURfr65mTZkSclDUwXAQAAAACA2agoiqRozXQ3khxiQURdV1MzIoo8LIjYuTFGsSt8qKs6dVGkqJOqrmaquwAAAAAAwH4c/DkWj0udekewUNdTG2fU1Y7ZD6lTp5raTKMozH8AAAAAAIBZ4JCaEZE8tEFGr9fN7bffnl63myOPODJLli1JdkQSO2OJ5GETJgAAAAAAgEPOITYj4qElmbZu3Zr77luT1XfdlSuvvDJbtmzJVACxs+HUr9rUCAAAAAAAOGQdckFEkvR63Wzbti1z58xJWRSZM2cw7XZfqmrHck11krpIUReJGREAAAAAAHDIOuSWZppSZGhoXo5beXzmzx/OsuVHpL9/IKmzY0mmOqnr1CmmQgkAAAAAAOCQdEgFEXWdVFWVsiwzPDycJFm6dFmKoki9W+JQJ6ny8L0iAAAAAACAQ88hFUQk9W6Bw84A4qFjU0sxFWWdFGXqupqZbgIAAAAAAAfkkNsjYmqv6ipF8dDG1TuDiKnfpnap3rFXNQAAAAAAcAg7xGZE7AgdiqlFl5KHwoikTFkmO+OHuq5TFMWudnC4m5ycyPaRbZn6B1JM1X8xlSVO/TPZPZqbmJjI2NhYjjjiiJTlIZc5AuyRsQ4AAAAOP4dgEJFMbUldJKl33VSo6+phG1O3dvxuTgRPHOvuuzuf/8w/JelL2R5M0RlIWp2MjY0nKTJncCDz5s1Nq1WmruuMbh/NmjVr86sv+pUMDg7MdPcBDoixDgAAAA4/h1gQUaQsytRFUpTlrgWYpmZF1Lt+T1GkSDn1p4VPP/LEUFW9jG7fmqpuJeVY7t3UzddvvSf3rl2farKbgb5OjjlycZ765FNy4rFHpVUmxx+/Mu12a/8PDnCIMNYBAADA4eeQCiKG5i3KwMDQbhMdHj7nod7t66n/K0q7RfDEUNd1ur1uJnvdfOuejbnm9k3ZOjKWbrebXrebrWPj2bBlS269c3UuPPtJOe/MVen1erttAA9wqDPWAQAAwOHnkAoiWq12Wq1Dqktw6KjrVFWV29dO5Bt3bE27bzDF2GQ6ZSdFWafqFul2e9k+UeXKb96UTpGcdsLRbs4Bs4uxDgAAAA477vrDLFEnGZus8s27RzLWS8pqMmWrnSXz5iR1NyOjE1n/4NbUVTIy3suNt92R5fPbqatqprsOcMCMdcwGdV0/Mvwqds3XzcPn8e5sN7XE6EO/AwAAPJEIImAW2Tw6mZGJKr2qSpWk3W6n02ml7vXSbrfS1+lLt9tNXfWyZWRbRrZtis8IA7ONsY5D3a233pJ3v+edqeukLMsURVKW7RRFkXLnnmZJJie76VW9FEkWLlqUS1/zugwODs5o3wEAAGaCIAJmkZGxbpIife12qqpKXSfbx7oZGmilrifTarVSlEWKop2+1mSqXjdxew6YZYx1HOrWb9iQz33u86mrKmVZpNPXmarAup4KJ4qdAUWR1FPVefSKo9Pr9Wa45wAAADNDEAGzRFEkfe1OinIinXYr3W5S9apsHa8yPplMTiYpy5R1UhR1Oq2kLN2YA2YXYx2zQVVVmZgYT1m2UpStTExMpK7rdNrtlGWZsizS12mnTp1et07qOt1uLwIzAADgiUoQAbNFUWbB3E7aqdMtyrRaZaqqSq9XZdtElTJ1imLq5kddT2ZuX9LX3rlCNcAsYaxjFiiStFvtlDvqs66nZkb0D/Rl2dIlWbJkSebMGci2bduz+q57s21kW9rtduypDgAAPFEJImCWKItk3tx2Vi0tc/OaiaQoU6ROXVWp697UtphFkarqZqicyPI5dfr7WlMfLwaYJYx1zAZlWaZsTe0NMWfOQOq6ztFHHpEVxxyV/v7+JFObVA8M9GdgYCCbt2xOXdc2qgaAhtR1nfoxJP5FUfj+DDBNBBEwSxRlMqevzBkrBtKdnMztD0xkotdL1auSup76eGbdzfzOZFYOV1k0ry+D/X0z3W2AR8VYx2xQ13VarXba7TJ9fX05+eQTMzx/fspy6mbGzpshvV6VasfvdTXTvQaAw1e328111/1ner3JtMoirbJMkWLq2rHI1IdadmzcVO+4piyLMiedfGbmzVsw090HeEIQRMAsUabIYH87i+YP5IwVVRb1j+XejduzcaSXblWlv11nuD9ZOreToYGBzBnsZO7QoA8JA7OKsY7ZoUiRpNft5viTT8r8eUNJ6lR1ndRF6qrO5ORktm0bydZt2zI+PpFOp20JMQBoTJ1t27ZkYmIs7dbOICIpW60kdaqqTlVXO7Zrmpo5URZlet3uDPYZ4IlFEAGzRNlqZWBwIO1OJ512X4YGR3P0ksGMjk2kV1dJkfSVSbtdptMuM9DfzoLh4R23SgBmB2Mds0VdJ32dTiYnJ3fNgKh3HJ+c7GbL5q1Zd/8D2bRpczqdTpYtXbbjZggA0Iw6RepUVZHRkZGsf2BdNm/ZmrJsZ8mSxVm0eGHa7XaKskzqOlNzJGzgBDBdBBEwSyxZdkx+8DkvSjI1lbSqpn7tXAdz1y24nbNPiyJlq5VOnyVLgNnDWMds0ev1Urfb2bZte5b0eqnrpNerMj4xkc2bt2XD+g1Zv2F96rrOqSedmKec9aS0WuVMdxsADltFUWRyYjLX/Od/5mtf+1oeWHd/UhRptzppdzpZtnxJnv7M783pT3pS2q1WWqbUAkwrQQTMEu12J/PmL5rpbgA0yljHbFC2ytR1lW7Vzcj27dm2dXt6vV62j45ly5ateXDTpmzbOpKyVeSUE4/Lk04/IXMH+xMzdwCgMWOjo/nHj/xjbr7pxkyOT6bX66VstdIqqhRFmbVr1uWfPvLRrF+/Ic+66FlTKzSZEAEwbQQRAADwKJRFkVarlW53KohYffc96XZ72b59NNtHt6fX7WXu4EBOPmlFTly5IgvmD2XO4FwxBAA0pKrqfPbTn80tN92Yvr6+9Ca76e8fSP/gQAYGhjJn3rKMbN2QLVs25D++/JWsPPbYHH/CCXIIgGkkiAAAgEehqqokSV1VGRsby8jItlRVndRJX6edo5Ytyspjl2fZkoVZuGAo8+YNZ3BwbuyqDgDNGB8fy60335SqV2d8dCx9fZ0cdfSR2T46mrqusn37hpRFnbIsM7p9LP/vP/5fjlqxYmpzJwCmhSACAAAehaqqUlXd9HVaaZVl6k4rfZ12Fsyfm2VLFmTh/KEMzZub+fOHMm/+cAbnLEhnoH+muw0Ah686qXq9TE5OpCiK1HWdNfeuTa/XS4oiRVGmKIr0er0USTY8sCHbR7ZPfZAAgGkhiAAAgEdh5cqVedUrX5VOu5V2q5V2u5VOu0yn097x/+10+jrp6+tLp68/fX3zMndoXsrSZtUA0JR2u51Wq52qqtLrVen1xlMURVIkRZ1UmdrQuiiSVqeTsbHJ1GZEAEwbQQQAADwKK1Yckxf+8otnuhsAwMMURZGyTOq6Tl3t+L0odqy+VO8IJYqkLDN//nB6vZ4gAmAaCSIAAAAAmMXqqWWYMhVIVKnSq6qppZeKIq2ynNqqqUjKsszS5cunvoj9mwCmi/nhAAAAAMxarXY7CxfMT3dyMkk9NRuirlNVVeqq3jXzoSzLLFqyKEuXLkt/f1/KUhABMF0EEQAAAADMWmVZ5NRTV2bhgnlTm1ZPdFNVdeo6qZNUVZXUdRYumJ+TTz4pCxfMz9CcfkEEwDSyNBMAAAAAs1an05fnPf+XctHFz8umTVsyOdmbWpZph1ZZpt1pZd78ocybNz/z5g9ncGAgc4fmzWCvAZ5YBBEAAAAAzFrtdidPevIFM90NAPZBEAFwgHq9XrZs3pR6bw3qOnV2bndWpNvrZmxsLEcccWT6+vqmq5sAABwA13YAANNHEAFwgMbHx3PNNf+Zbq+XnSuJ7vzhtK7rFEWROlPrkD5w/wP5t3/7XDZsXJ93veu9Oe64lTPXcQAAHsG1HQDA9BFEABywemrDs6pKXRQ7fkotk6Ke+iE1SV3Vuf6GG/LZz3wm69dvzMJFw7utTQoAwKHCtR0AwHQRRAAcsCJ1PfUJuRRJkWLHD6pFylYrIyPbc8UVX8rXvvrVjI+PpyzLlEVrpjsNAMAeubYDAJguggiAR6FOUiUp6nq3g2vX3Z+PfvSfcued30mv101VVWm32xnZvj11Xc1UdwEA2AfXdgAA00MQAXDAdvyAWhc7dy1M1evlhutvzMf/+Z/z4MaNKcpW6rpKXdcpy3LHKcXeHhAAgBnj2g4AYLoIIgAehbqup6bvV0VGx8byxS9ekS9e8cWMjo2lKIqk6k1N7S+KdLvddLvdqfYAABxyXNsBAEwPQQTAAarrIkmZJNm4cWM+/vGP56abbkq3201RFFO/yiJ1suuH1E530g+rAACHINd2AADTRxABcICKok5R1HnwwQfzwQ9+MKtXr06dOkVZpiyKlGWZbq+XXtVNUqTVak19kg4AgEOOazsAgOkjiAB4FOq6zte//vWsWbMmZatIkTKtVju9Xi+Tk5OpqiplWaZslelVVXqT3dTxqTkA4PHrdSczOb5tx3JCyc49Duq6Tuqk2rGXwc4P7E/ta9DKvAWLUxTljPX7UObaDgBgeggiAA5YkZGRkdx8881JqrTbrdR1ncnJiUxMTKYoinQ6ndR1ne5kN1XqpGM7QwDg4JgY25IH7r4pvd7UMkF11Utd11M3yLu9dLvd9Koq3W6VXlWl7lXpG5iTp1z4w2m1BRGP5NoOAGC6CCIADtC2bVtz+eWXZ926dVOfiuvt/OG/m1a7lXarPTV9v9dL4odUAODgqus6VdVNnSpFUaZXTX3d7dXp9epUdVLXO9tVSaY2Yvb5/T1zbQcAMH0EEQAHaGJiIvc/sDZVPbWBYavVSpK0Wu10u72MT06krqZ+1C+KInWdHesI+7EVAHj8ihQp6iJlymzZOpIN6zek02llztyhHZ/cT4qiTFnWadX11I31me70Icy1HQDA9BFEABygVquV5cuPyOjo9h2fmEtS16mqqR9k6yJTSzUXRYokrbLM0NC8dDqdme04AHDYKFKmrqts3759x74Fde66a3WWLVuauXPnJpmaETE2NpaqqlKUrkP2xrUdAMD0EUQAHKDly5fnb/767zK1zMF3/+nDDzz0KbmiKDI0NDQNvQMAngjqJFVdZ+nSpVObVCcZGhrK+vXrMzg4mKIosmHDhmzYsDFz5w6lbA/EtIg9c20HADB9BBEAB6gsWxkeHp7pbgAAT1B1nVRVlcleL62yTFVVmZiYzMREN0XRSrfbS6evL61OX4455ti02+0MDA6lsJLQHrm2AwCYPoIIAACAWaFOteOj++Pj4/n2t7+dkZHRDA8PZ+7cuamTjI6NpdVqZ/vo9gzPH96xpwEAAMwsQQQAAMBsUUztbdAqy6xatSp1nbTbnVRVlV6vl23btmV8dDR9fX0pW+2kLGe6xwAAIIgAAACYNYoiKZIyZQYG5qTb7aZX10lZpJVW5s2bl4GBgdR1nW6vysP3NwAAgJni4zEAAACzQVGkSFLUdVInRYoURZEy2W0JpqKYOl4XSW2nagAADgGCCAAAgFmiKIoUKVLVdarUU+FEWSZ1Utd16rqeCiWKImVR2KgaAIBDgiACAABglijLMnVZpN414yGpMhVKVEVSFdkVQrRbZcqijOWZAACYafaIAAAAmAXKspV230CKqkpV1anqKnW7TlVNLb9U13WqqkpVVSmS9HpV2n0DcggAAGacIAIAAGAW6J8zP0cef06S7Nj5Ycf+D3vZBqLOVAZRlq3mOwcAAPsgiAAAAJgFiqJM0bK6LgAAs4+rWAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDGCCAAAAAAAoDHtR9O4KIqm+sEsMRM1oO6Y7hpQcyTqjunneywzwVjHdDPWMROMdcwEdcd08z2WmfBoaqDYtGlTfSANh4eHH3OHOPyMj49Py/P09/dPy/MwO0xH3ak5vpu6Y7r5HstMMNYx3Yx1zARjHTNB3THdfI9lJmzevHm/bR5VEDFdhQwAAAAAABza+vv7DyiIsEcEAAAAAADQGEEEAAAAAADQGEEEAAAAAADQGEEEAAAAAADQGEEEAAAAAADQGEHEw7znPe/JKaeckuHh4Vx44YX5j//4j322//KXv5wLL7www8PDOfXUU/O+971vmnrK4eIrX/lKfvInfzLHH398BgYG8i//8i/7PUfdcTAY75hOxjpmirGOmaDumG5qjunm2o7ppuaYCeru4BNE7PCRj3wkr371q/Pa1742V111VZ7+9Kfnuc99blavXr3H9nfeeWee97zn5elPf3quuuqqXHrppXnlK1+Zj33sY9Pcc2az7du358wzz8yf//mfH1B7dcfBYLxjuhnrmAnGOmaCumO6qTlmgms7ppuaYyaou4Ov2LRpU30gDYeHhzM+Pt50f2bMM57xjDzlKU/JZZddtuvYWWedlec85zl585vf/Ij2b3zjG/PJT34y11133a5jv/Ebv5EbbrghX/rSl6alzxxeBgYG8uEPfzg//uM/vtc26o6DwXjHTDLWMV2MdcwEdcd0U3PMNNd2TDc1x0xQd/vW39+fzZs377edGRFJJiYmcs011+Tiiy/e7fjFF1+cK6+8co/nXHnllY9o/wM/8AP5xje+kcnJycb6yhObuuPxMt4xG6g5Hi9jHTNB3THd1Byzhbpjuqk5ZoK62z9BRJL169en1+tl2bJlux1ftmxZ1q1bt8dz1q1bt8f23W4369evb6yvPLGpOx4v4x2zgZrj8TLWMRPUHdNNzTFbqDumm5pjJqi7/RNEPExRFLt9Xdf1I47tr/2ejsPBpO44GIx3HOrUHAeDsY6ZoO6YbmqO2UDdMd3UHDNB3e2bICLJkiVL0mq1HvGpkQceeOARSdZOy5cv32P7drudxYsXN9ZXntjUHY+X8Y7ZQM3xeBnrmAnqjumm5pgt1B3TTc0xE9Td/gkikvT19eWcc87JF77whd2Of+ELX8jTnva0PZ7ztKc97RHtP//5z+fcc89Np9NprK88sak7Hi/jHbOBmuPxMtYxE9Qd003NMVuoO6abmmMmqLv9E0Ts8LKXvSx/9Vd/lcsvvzy33HJLXvOa1+Tuu+/Oi170oiTJb/3Wb+WFL3zhrva/8iu/ktWrV+fSSy/NLbfckssvvzyXX355XvGKV8zQK2A22rZtW6677rpcd911SZLvfOc7ue6667J69eok6o5mGO+YbsY6ZoKxjpmg7phuao6Z4NqO6abmmAnq7uBrz3QHDhXPf/7zs3HjxvzRH/1R1q5dmzPOOCMf//jHc9xxxyVJ1q5dm7vvvntX++OPPz4f//jHc+mll+bd7353jjzyyLzlLW/JT/zET8zUS2AW+sY3vpEf+qEf2vX1pZdemiT5+Z//+bz//e9XdzTCeMd0M9YxE4x1zAR1x3RTc8wE13ZMNzXHTFB3B1+xadOm+kAaDg8PZ3x8vOn+AAAAAAAAs0B/f382b96833aWZgIAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABojiAAAAAAAABrTfjSN+/v7m+oHAAAAAABwGDrgIGLz5s1N9gMAAAAAADgMWZoJAAAAAABojCACAAAAAABojCACAAAAAABojCACAAAAAABojCACAAAAAABojCACAAAAAABojCACAAAAAABojCACAAAAAABozP8P2U18QibjUUUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 2000x400 with 11 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_digits_with_boxes(train, training_labels, training_labels, np.array([]), training_bboxes, np.array([]), \"training digits and their labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c640af4-816b-47d6-ae66-362f7b9d58a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset element_spec={'images': TensorSpec(shape=(1000, 1000, 3), dtype=tf.uint8, name=None), 'labels': TensorSpec(shape=(), dtype=tf.uint8, name=None), 'bounding_boxe': TensorSpec(shape=(4,), dtype=tf.uint8, name=None)}>\n"
     ]
    }
   ],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices({'images': np.uint8(train), 'labels': np.uint8(training_labels), 'bounding_boxe': np.uint8(training_bboxes)})\n",
    "print(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61b0e94d-e43e-4c21-af6d-a721a69e3466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset element_spec={'images': TensorSpec(shape=(1000, 1000, 3), dtype=tf.uint8, name=None), 'labels': TensorSpec(shape=(), dtype=tf.uint8, name=None), 'bounding_boxe': TensorSpec(shape=(4,), dtype=tf.uint8, name=None)}>\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = tf.data.Dataset.from_tensor_slices({'images': np.uint8(validate), 'labels': np.uint8(validation_labels), 'bounding_boxe': np.uint8(validation_bboxes)})\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d51570bb-4232-4784-b280-07f36ea00106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " images (InputLayer)            [(None, 500, 500, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 498, 498, 16  448         ['images[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " average_pooling2d_23 (AverageP  (None, 124, 124, 16  0          ['conv2d_23[0][0]']              \n",
      " ooling2D)                      )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 122, 122, 32  4640        ['average_pooling2d_23[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " average_pooling2d_24 (AverageP  (None, 30, 30, 32)  0           ['conv2d_24[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 28, 28, 32)   9248        ['average_pooling2d_24[0][0]']   \n",
      "                                                                                                  \n",
      " average_pooling2d_25 (AverageP  (None, 7, 7, 32)    0           ['conv2d_25[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 5, 5, 64)     18496       ['average_pooling2d_25[0][0]']   \n",
      "                                                                                                  \n",
      " average_pooling2d_26 (AverageP  (None, 1, 1, 64)    0           ['conv2d_26[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 64)           0           ['average_pooling2d_26[0][0]']   \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 128)          8320        ['flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " labels (Dense)                 (None, 1)            129         ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " bounding_box (Dense)           (None, 4)            516         ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 41,797\n",
      "Trainable params: 41,797\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Feature extractor is the CNN that is made up of convolution and pooling layers.\n",
    "'''\n",
    "def feature_extractor(inputs):\n",
    "    x = tf.keras.layers.Conv2D(16, activation='relu', kernel_size=3, input_shape=(500, 500, 3))(inputs)\n",
    "    x = tf.keras.layers.AveragePooling2D((4, 4))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32,kernel_size=3,activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((4, 4))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32,kernel_size=3,activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((4, 4))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(64,kernel_size=3,activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((4, 4))(x)\n",
    "\n",
    "\n",
    "    return x\n",
    "\n",
    "'''\n",
    "dense_layers adds a flatten and dense layer.\n",
    "This will follow the feature extraction layers\n",
    "'''\n",
    "def dense_layers(inputs):\n",
    "  x = tf.keras.layers.Flatten()(inputs)\n",
    "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "  return x\n",
    "\n",
    "\n",
    "'''\n",
    "Classifier defines the classification output.\n",
    "This has a set of fully connected layers and a softmax layer.\n",
    "'''\n",
    "def classifier(inputs):\n",
    "\n",
    "  classification_output = tf.keras.layers.Dense(1, activation='sigmoid', name = 'labels')(inputs)\n",
    "  return classification_output\n",
    "\n",
    "\n",
    "'''\n",
    "This function defines the regression output for bounding box prediction. \n",
    "Note that we have four outputs corresponding to (xmin, ymin, xmax, ymax)\n",
    "'''\n",
    "def bounding_box_regression(inputs):\n",
    "    bounding_box_regression_output = tf.keras.layers.Dense(units = '4', name = 'bounding_box')(inputs)\n",
    "    return bounding_box_regression_output\n",
    "\n",
    "\n",
    "def final_model(inputs):\n",
    "    feature_cnn = feature_extractor(inputs)\n",
    "    dense_output = dense_layers(feature_cnn)\n",
    "\n",
    "    '''\n",
    "    The model branches here.  \n",
    "    The dense layer's output gets fed into two branches:\n",
    "    classification_output and bounding_box_output\n",
    "    '''\n",
    "    classification_output = classifier(dense_output)\n",
    "    bounding_box_output = bounding_box_regression(dense_output)\n",
    "\n",
    "    model = tf.keras.Model(inputs = inputs, outputs = [classification_output, bounding_box_output])\n",
    "\n",
    "    return model\n",
    "  \n",
    "\n",
    "def define_and_compile_model(inputs):\n",
    "  model = final_model(inputs)\n",
    "  \n",
    "  model.compile(optimizer='adam', \n",
    "              loss = {'labels' : 'binary_crossentropy',\n",
    "                      'bounding_box' : 'mse'\n",
    "                     },\n",
    "              metrics = {'labels' : 'accuracy',\n",
    "                         'bounding_box' : 'mse'\n",
    "                        })\n",
    "  return model\n",
    "\n",
    "    \n",
    "with strategy.scope():\n",
    "  inputs = tf.keras.layers.Input(shape=(500, 500, 3), name=\"images\")\n",
    "  model = define_and_compile_model(inputs)\n",
    "\n",
    "# print model layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f1e55bc-c640-473d-945c-956c9bd24ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_y(labels, bounding_boxes):\n",
    "    return labels, bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1a1c5b6-8f61-47a6-a66f-0ede87cd9b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-27 10:48:41.149231: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 73953.6094 - labels_loss: 0.6798 - bounding_box_loss: 73952.9219 - labels_accuracy: 0.6250 - bounding_box_mse: 73952.9219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-27 10:48:43.178353: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step - loss: 73953.6094 - labels_loss: 0.6798 - bounding_box_loss: 73952.9219 - labels_accuracy: 0.6250 - bounding_box_mse: 73952.9219 - val_loss: 59765.9648 - val_labels_loss: 0.6945 - val_bounding_box_loss: 59765.2695 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 59765.2695\n",
      "Epoch 2/45\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 73901.4297 - labels_loss: 0.6946 - bounding_box_loss: 73900.7344 - labels_accuracy: 0.3750 - bounding_box_mse: 73900.7344 - val_loss: 59723.4727 - val_labels_loss: 0.7006 - val_bounding_box_loss: 59722.7734 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 59722.7773\n",
      "Epoch 3/45\n",
      "1/1 [==============================] - 0s 454ms/step - loss: 73853.7812 - labels_loss: 0.7019 - bounding_box_loss: 73853.0781 - labels_accuracy: 0.3750 - bounding_box_mse: 73853.0781 - val_loss: 59661.9180 - val_labels_loss: 0.7035 - val_bounding_box_loss: 59661.2188 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 59661.2188\n",
      "Epoch 4/45\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 73784.9688 - labels_loss: 0.7053 - bounding_box_loss: 73784.2656 - labels_accuracy: 0.3750 - bounding_box_mse: 73784.2656 - val_loss: 59568.2148 - val_labels_loss: 0.7058 - val_bounding_box_loss: 59567.5078 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 59567.5078\n",
      "Epoch 5/45\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 73680.1094 - labels_loss: 0.7078 - bounding_box_loss: 73679.3906 - labels_accuracy: 0.3750 - bounding_box_mse: 73679.3906 - val_loss: 59429.3047 - val_labels_loss: 0.7076 - val_bounding_box_loss: 59428.5977 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 59428.5977\n",
      "Epoch 6/45\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 73524.4219 - labels_loss: 0.7095 - bounding_box_loss: 73523.7109 - labels_accuracy: 0.3750 - bounding_box_mse: 73523.7109 - val_loss: 59229.4883 - val_labels_loss: 0.7072 - val_bounding_box_loss: 59228.7852 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 59228.7852\n",
      "Epoch 7/45\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 73300.4531 - labels_loss: 0.7091 - bounding_box_loss: 73299.7422 - labels_accuracy: 0.3750 - bounding_box_mse: 73299.7422 - val_loss: 58947.2383 - val_labels_loss: 0.7026 - val_bounding_box_loss: 58946.5352 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 58946.5352\n",
      "Epoch 8/45\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 72983.9688 - labels_loss: 0.7037 - bounding_box_loss: 72983.2656 - labels_accuracy: 0.3750 - bounding_box_mse: 72983.2656 - val_loss: 58554.0273 - val_labels_loss: 0.6929 - val_bounding_box_loss: 58553.3398 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 58553.3398\n",
      "Epoch 9/45\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 72543.0859 - labels_loss: 0.6917 - bounding_box_loss: 72542.3906 - labels_accuracy: 0.6250 - bounding_box_mse: 72542.3906 - val_loss: 58017.4883 - val_labels_loss: 0.6765 - val_bounding_box_loss: 58016.8086 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 58016.8125\n",
      "Epoch 10/45\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 71941.0625 - labels_loss: 0.6704 - bounding_box_loss: 71940.3906 - labels_accuracy: 0.6250 - bounding_box_mse: 71940.3906 - val_loss: 57288.3555 - val_labels_loss: 0.6751 - val_bounding_box_loss: 57287.6797 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 57287.6797\n",
      "Epoch 11/45\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 71122.1875 - labels_loss: 0.6620 - bounding_box_loss: 71121.5234 - labels_accuracy: 0.6250 - bounding_box_mse: 71121.5234 - val_loss: 56297.9922 - val_labels_loss: 0.7172 - val_bounding_box_loss: 56297.2734 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 56297.2773\n",
      "Epoch 12/45\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 70008.5938 - labels_loss: 0.6948 - bounding_box_loss: 70007.8984 - labels_accuracy: 0.6250 - bounding_box_mse: 70007.8984 - val_loss: 54980.2031 - val_labels_loss: 0.8223 - val_bounding_box_loss: 54979.3828 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 54979.3828\n",
      "Epoch 13/45\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 68525.1797 - labels_loss: 0.7888 - bounding_box_loss: 68524.3906 - labels_accuracy: 0.6250 - bounding_box_mse: 68524.3906 - val_loss: 53257.2734 - val_labels_loss: 1.0143 - val_bounding_box_loss: 53256.2578 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 53256.2578\n",
      "Epoch 14/45\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 66581.4609 - labels_loss: 0.9684 - bounding_box_loss: 66580.4922 - labels_accuracy: 0.6250 - bounding_box_mse: 66580.4922 - val_loss: 51028.1289 - val_labels_loss: 1.3084 - val_bounding_box_loss: 51026.8203 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 51026.8203\n",
      "Epoch 15/45\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 64060.3984 - labels_loss: 1.2471 - bounding_box_loss: 64059.1484 - labels_accuracy: 0.6250 - bounding_box_mse: 64059.1484 - val_loss: 48189.0586 - val_labels_loss: 1.7171 - val_bounding_box_loss: 48187.3438 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 48187.3438\n",
      "Epoch 16/45\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 60838.5742 - labels_loss: 1.6358 - bounding_box_loss: 60836.9375 - labels_accuracy: 0.6250 - bounding_box_mse: 60836.9375 - val_loss: 44650.3789 - val_labels_loss: 2.2286 - val_bounding_box_loss: 44648.1484 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 44648.1484\n",
      "Epoch 17/45\n",
      "1/1 [==============================] - 0s 466ms/step - loss: 56804.5898 - labels_loss: 2.1227 - bounding_box_loss: 56802.4688 - labels_accuracy: 0.6250 - bounding_box_mse: 56802.4688 - val_loss: 40333.6484 - val_labels_loss: 2.8136 - val_bounding_box_loss: 40330.8359 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 40330.8359\n",
      "Epoch 18/45\n",
      "1/1 [==============================] - 0s 492ms/step - loss: 51851.9609 - labels_loss: 2.6804 - bounding_box_loss: 51849.2812 - labels_accuracy: 0.6250 - bounding_box_mse: 51849.2812 - val_loss: 35197.8945 - val_labels_loss: 3.5098 - val_bounding_box_loss: 35194.3828 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 35194.3828\n",
      "Epoch 19/45\n",
      "1/1 [==============================] - 1s 501ms/step - loss: 45902.6992 - labels_loss: 3.3438 - bounding_box_loss: 45899.3555 - labels_accuracy: 0.6250 - bounding_box_mse: 45899.3555 - val_loss: 29278.6914 - val_labels_loss: 4.3170 - val_bounding_box_loss: 29274.3750 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 29274.3750\n",
      "Epoch 20/45\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 38943.2422 - labels_loss: 4.1150 - bounding_box_loss: 38939.1250 - labels_accuracy: 0.6250 - bounding_box_mse: 38939.1250 - val_loss: 22901.1250 - val_labels_loss: 5.2193 - val_bounding_box_loss: 22895.9043 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 22895.9062\n",
      "Epoch 21/45\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 31258.3711 - labels_loss: 4.9751 - bounding_box_loss: 31253.3965 - labels_accuracy: 0.6250 - bounding_box_mse: 31253.3965 - val_loss: 16654.2559 - val_labels_loss: 6.2023 - val_bounding_box_loss: 16648.0527 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 16648.0527\n",
      "Epoch 22/45\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 23375.9727 - labels_loss: 5.9123 - bounding_box_loss: 23370.0586 - labels_accuracy: 0.6250 - bounding_box_mse: 23370.0586 - val_loss: 11750.2451 - val_labels_loss: 7.2497 - val_bounding_box_loss: 11742.9951 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 11742.9951\n",
      "Epoch 23/45\n",
      "1/1 [==============================] - 0s 471ms/step - loss: 16453.9648 - labels_loss: 6.9101 - bounding_box_loss: 16447.0527 - labels_accuracy: 0.6250 - bounding_box_mse: 16447.0527 - val_loss: 10257.2900 - val_labels_loss: 8.2841 - val_bounding_box_loss: 10249.0059 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 10249.0059\n",
      "Epoch 24/45\n",
      "1/1 [==============================] - 0s 478ms/step - loss: 12537.9141 - labels_loss: 7.8966 - bounding_box_loss: 12530.0186 - labels_accuracy: 0.6250 - bounding_box_mse: 12530.0176 - val_loss: 14671.7568 - val_labels_loss: 9.0779 - val_bounding_box_loss: 14662.6797 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 14662.6797\n",
      "Epoch 25/45\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 14287.0527 - labels_loss: 8.6541 - bounding_box_loss: 14278.3975 - labels_accuracy: 0.6250 - bounding_box_mse: 14278.3975 - val_loss: 23328.5410 - val_labels_loss: 8.9544 - val_bounding_box_loss: 23319.5879 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 23319.5879\n",
      "Epoch 26/45\n",
      "1/1 [==============================] - 1s 573ms/step - loss: 20803.6055 - labels_loss: 8.5381 - bounding_box_loss: 20795.0664 - labels_accuracy: 0.6250 - bounding_box_mse: 20795.0664 - val_loss: 27801.1992 - val_labels_loss: 7.3766 - val_bounding_box_loss: 27793.8242 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 27793.8242\n",
      "Epoch 27/45\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 24495.2305 - labels_loss: 7.0369 - bounding_box_loss: 24488.1934 - labels_accuracy: 0.6250 - bounding_box_mse: 24488.1914 - val_loss: 25772.4648 - val_labels_loss: 4.9108 - val_bounding_box_loss: 25767.5547 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 25767.5547\n",
      "Epoch 28/45\n",
      "1/1 [==============================] - 0s 471ms/step - loss: 22824.8047 - labels_loss: 4.6898 - bounding_box_loss: 22820.1152 - labels_accuracy: 0.6250 - bounding_box_mse: 22820.1152 - val_loss: 20516.0059 - val_labels_loss: 2.2587 - val_bounding_box_loss: 20513.7461 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 20513.7480\n",
      "Epoch 29/45\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 18597.2930 - labels_loss: 2.1624 - bounding_box_loss: 18595.1289 - labels_accuracy: 0.6250 - bounding_box_mse: 18595.1289 - val_loss: 15281.0654 - val_labels_loss: 0.7647 - val_bounding_box_loss: 15280.3008 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 15280.3018\n",
      "Epoch 30/45\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 14701.6123 - labels_loss: 0.7609 - bounding_box_loss: 14700.8516 - labels_accuracy: 0.3750 - bounding_box_mse: 14700.8516 - val_loss: 11724.0811 - val_labels_loss: 2.7806 - val_bounding_box_loss: 11721.3008 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 11721.3008\n",
      "Epoch 31/45\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 12535.6289 - labels_loss: 2.8730 - bounding_box_loss: 12532.7559 - labels_accuracy: 0.3750 - bounding_box_mse: 12532.7559 - val_loss: 10071.8242 - val_labels_loss: 3.9472 - val_bounding_box_loss: 10067.8770 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 10067.8770\n",
      "Epoch 32/45\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 12150.5898 - labels_loss: 4.0922 - bounding_box_loss: 12146.4980 - labels_accuracy: 0.3750 - bounding_box_mse: 12146.4980 - val_loss: 9796.7246 - val_labels_loss: 4.3324 - val_bounding_box_loss: 9792.3926 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 9792.3926\n",
      "Epoch 33/45\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 12922.2520 - labels_loss: 4.4955 - bounding_box_loss: 12917.7559 - labels_accuracy: 0.3750 - bounding_box_mse: 12917.7559 - val_loss: 10215.5605 - val_labels_loss: 4.2650 - val_bounding_box_loss: 10211.2969 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 10211.2969\n",
      "Epoch 34/45\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 14133.1045 - labels_loss: 4.4262 - bounding_box_loss: 14128.6777 - labels_accuracy: 0.3750 - bounding_box_mse: 14128.6777 - val_loss: 10797.2256 - val_labels_loss: 3.9282 - val_bounding_box_loss: 10793.2988 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 10793.2988\n",
      "Epoch 35/45\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 15255.7637 - labels_loss: 4.0753 - bounding_box_loss: 15251.6875 - labels_accuracy: 0.3750 - bounding_box_mse: 15251.6875 - val_loss: 11227.8936 - val_labels_loss: 3.4109 - val_bounding_box_loss: 11224.4824 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 11224.4824\n",
      "Epoch 36/45\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 15996.8125 - labels_loss: 3.5358 - bounding_box_loss: 15993.2764 - labels_accuracy: 0.3750 - bounding_box_mse: 15993.2764 - val_loss: 11371.2598 - val_labels_loss: 2.7471 - val_bounding_box_loss: 11368.5127 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 11368.5127\n",
      "Epoch 37/45\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 16244.0576 - labels_loss: 2.8428 - bounding_box_loss: 16241.2139 - labels_accuracy: 0.3750 - bounding_box_mse: 16241.2139 - val_loss: 11213.5176 - val_labels_loss: 1.9513 - val_bounding_box_loss: 11211.5664 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 11211.5664\n",
      "Epoch 38/45\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 16005.9404 - labels_loss: 2.0111 - bounding_box_loss: 16003.9297 - labels_accuracy: 0.3750 - bounding_box_mse: 16003.9297 - val_loss: 10823.5264 - val_labels_loss: 1.1116 - val_bounding_box_loss: 10822.4150 - val_labels_accuracy: 0.4000 - val_bounding_box_mse: 10822.4150\n",
      "Epoch 39/45\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 15370.8359 - labels_loss: 1.1309 - bounding_box_loss: 15369.7061 - labels_accuracy: 0.3750 - bounding_box_mse: 15369.7051 - val_loss: 10328.5508 - val_labels_loss: 0.6877 - val_bounding_box_loss: 10327.8633 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 10327.8633\n",
      "Epoch 40/45\n",
      "1/1 [==============================] - 0s 471ms/step - loss: 14484.5977 - labels_loss: 0.6731 - bounding_box_loss: 14483.9248 - labels_accuracy: 0.6250 - bounding_box_mse: 14483.9248 - val_loss: 9894.6572 - val_labels_loss: 1.0814 - val_bounding_box_loss: 9893.5762 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 9893.5762\n",
      "Epoch 41/45\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 13534.3027 - labels_loss: 1.0381 - bounding_box_loss: 13533.2646 - labels_accuracy: 0.6250 - bounding_box_mse: 13533.2646 - val_loss: 9699.4668 - val_labels_loss: 1.7457 - val_bounding_box_loss: 9697.7217 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 9697.7217\n",
      "Epoch 42/45\n",
      "1/1 [==============================] - 1s 654ms/step - loss: 12725.2959 - labels_loss: 1.6706 - bounding_box_loss: 12723.6250 - labels_accuracy: 0.6250 - bounding_box_mse: 12723.6250 - val_loss: 9888.7881 - val_labels_loss: 2.3574 - val_bounding_box_loss: 9886.4307 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 9886.4307\n",
      "Epoch 43/45\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 12242.3701 - labels_loss: 2.2534 - bounding_box_loss: 12240.1162 - labels_accuracy: 0.6250 - bounding_box_mse: 12240.1162 - val_loss: 10512.0664 - val_labels_loss: 2.8258 - val_bounding_box_loss: 10509.2402 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 10509.2402\n",
      "Epoch 44/45\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 12188.3896 - labels_loss: 2.6997 - bounding_box_loss: 12185.6904 - labels_accuracy: 0.6250 - bounding_box_mse: 12185.6895 - val_loss: 11459.4844 - val_labels_loss: 3.0932 - val_bounding_box_loss: 11456.3926 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 11456.3926\n",
      "Epoch 45/45\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 12520.2061 - labels_loss: 2.9546 - bounding_box_loss: 12517.2500 - labels_accuracy: 0.6250 - bounding_box_mse: 12517.2500 - val_loss: 12454.5508 - val_labels_loss: 3.1107 - val_bounding_box_loss: 12451.4414 - val_labels_accuracy: 0.6000 - val_bounding_box_mse: 12451.4414\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 45 # 45\n",
    "steps_per_epoch = 24//BATCH_SIZE  # 60,000 items in this dataset\n",
    "validation_steps = 1\n",
    "\n",
    "history = model.fit(train, y = format_y(training_labels, training_bboxes), epochs=EPOCHS, validation_data=(validate, [validation_labels, validation_bboxes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa855d15-657d-46fb-b4e0-4e3d51009ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
