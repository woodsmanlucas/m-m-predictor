{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e72e217d-28fd-4c03-ba01-7dd372b7c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b7a3c97-984e-49cd-a7e6-a0acfa91f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be updated\n",
    "root_dir = '/Users/lucas/Data_Science_Portfolio/M&M project/M&M_images_cropped'\n",
    "\n",
    "validation_dir = os.path.join(root_dir, 'validation')\n",
    "training_dir = os.path.join(root_dir, 'training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9df6f33-b68c-45d0-952f-991d3d9c9e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(validation_dir)\n",
    "os.makedirs(training_dir)\n",
    "\n",
    "os.makedirs(os.path.join(training_dir, 'M&Ms'))\n",
    "os.makedirs(os.path.join(training_dir, 'Not_M&Ms'))\n",
    "\n",
    "os.makedirs(os.path.join(validation_dir, 'M&Ms'))\n",
    "os.makedirs(os.path.join(validation_dir, 'Not_M&Ms'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2265a798-b9e5-404e-8e64-c75d26b9a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_and_ms = os.path.join(root_dir, 'M&Ms')\n",
    "\n",
    "split_size = 0.8\n",
    "number_of_m_and_ms = len(os.listdir(m_and_ms))\n",
    "number_of_training = round(number_of_m_and_ms*split_size)\n",
    "m_and_ms_training = os.path.join(training_dir, 'M&Ms')\n",
    "m_and_ms_validation = os.path.join(validation_dir, 'M&Ms')\n",
    "                         \n",
    "for i in os.listdir(m_and_ms)[:number_of_training]:\n",
    "    if(os.path.getsize(os.path.join(m_and_ms, i))):\n",
    "      copyfile(os.path.join(m_and_ms, i), os.path.join(m_and_ms_training, i))\n",
    "                                 \n",
    "for j in os.listdir(m_and_ms)[number_of_training:]:\n",
    "    if(os.path.getsize(os.path.join(m_and_ms, i))):\n",
    "        copyfile(os.path.join(m_and_ms, j), os.path.join(m_and_ms_validation, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72070c81-2aee-4e2f-b1d4-9873ed81219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nots = os.path.join(root_dir, 'Not_M&Ms')\n",
    "\n",
    "not_training = os.path.join(training_dir, 'Not_M&Ms')\n",
    "not_validation = os.path.join(validation_dir, 'Not_M&Ms')\n",
    "number_of_nots = len(os.listdir(nots))\n",
    "number_of_training = round(number_of_nots*split_size)\n",
    "\n",
    "for i in os.listdir(nots)[:number_of_training]:\n",
    "    if(os.path.getsize(os.path.join(nots, i))):\n",
    "        copyfile(os.path.join(nots, i), os.path.join(not_training, i))\n",
    "                       \n",
    "for j in os.listdir(nots)[number_of_training:]:\n",
    "    if(os.path.getsize(os.path.join(nots, j))):\n",
    "        copyfile(os.path.join(nots, j), os.path.join(not_validation, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0556aa56-d998-4b67-a301-d13952bb516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 images belonging to 2 classes.\n",
      "Found 6 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale= 1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory=training_dir,\n",
    "                                                   batch_size=64,\n",
    "                                                   class_mode='binary',\n",
    "                                                   target_size=(100, 100))\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(directory=validation_dir,\n",
    "                                                              batch_size=64,\n",
    "                                                              class_mode='binary',\n",
    "                                                              target_size=(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34e467bc-d840-4c0e-9452-f6a86bd0cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model = tf.keras.models.Sequential([ \n",
    "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(100, 100, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2), \n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(), \n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'), \n",
    "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "    \n",
    "    \n",
    "model.compile(optimizer=RMSprop(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ac1bb9e-a68f-48ee-92f4-eb42a39be6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 18:04:12.968467: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-27 18:04:13.259358: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 786ms/step - loss: 0.6788 - accuracy: 0.6667 - val_loss: 3.2410 - val_accuracy: 0.6667\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0168 - accuracy: 0.6250 - val_loss: 8.5753 - val_accuracy: 0.3333\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 18:04:13.693038: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 69ms/step - loss: 7.2518 - accuracy: 0.3750 - val_loss: 1.2961 - val_accuracy: 0.3333\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.2384 - accuracy: 0.3750 - val_loss: 0.5801 - val_accuracy: 0.6667\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5185 - accuracy: 0.6667 - val_loss: 0.4207 - val_accuracy: 0.8333\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5375 - accuracy: 0.7083 - val_loss: 1.2535 - val_accuracy: 0.6667\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7346 - accuracy: 0.6250 - val_loss: 0.3714 - val_accuracy: 0.8333\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4813 - accuracy: 0.7917 - val_loss: 0.3146 - val_accuracy: 0.6667\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2073 - accuracy: 0.9583 - val_loss: 0.1723 - val_accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1383 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0911 - accuracy: 1.0000 - val_loss: 0.1122 - val_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0589 - accuracy: 1.0000 - val_loss: 0.8032 - val_accuracy: 0.6667\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.1235 - accuracy: 1.0000 - val_loss: 0.3797 - val_accuracy: 0.8333\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3714 - accuracy: 0.7917 - val_loss: 1.2465 - val_accuracy: 0.6667\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1956 - accuracy: 0.9167 - val_loss: 0.1569 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, \n",
    "                    epochs=15, \n",
    "                    verbose=1, \n",
    "                    validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b4ca3c4-c672-4f5a-85ee-251dc3d8b536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://44e4a438-9561-46b0-863c-5ec4bb33b781/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://44e4a438-9561-46b0-863c-5ec4bb33b781/assets\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04dfef0-6d8c-4121-842c-5d1d7557c42e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
